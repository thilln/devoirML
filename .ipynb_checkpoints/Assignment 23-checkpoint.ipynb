{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 2 & 3   \n",
    "# Paul Thillen, Louis-Philippe Noël\n",
    "# 3. PRACTICAL PART (40 pts) : Neural network implementation and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerically stable softmax. \n",
    "You will need to compute a numerically\n",
    "stable softmax. Refer to posted readings to see how to do this. Start by\n",
    "writing the expression for a single vector, then adapt it for a mini-batch\n",
    "of examples stored in a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul Thillen\\Anaconda2\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:161: UserWarning: pylab import has clobbered these variables: ['copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import gzip,pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#two moons \n",
    "two_moons = np.loadtxt(open('2moons.txt','r'))\n",
    "#mnist\n",
    "f=gzip.open('mnist.pkl.gz')\n",
    "mnist=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    maximum = np.max(x, axis=-1, keepdims=True)\n",
    "    top = np.exp(x - maximum)\n",
    "    bottom = np.sum(top, axis=-1, keepdims=True)\n",
    "    return top/bottom\n",
    "\n",
    "#test\n",
    "#A= array([[ 1.,  3.,  4.,  5.,  6.],\n",
    "#       [ 4.,  5.,  6.,  3.,  3.],\n",
    "#       [ 1.,  3.,  4.,  5.,  6.],\n",
    "#       [ 4.,  5.,  6.,  3.,  3.]])\n",
    "#softmax(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniforme(a,b):\n",
    "    return(a+(b-a)*np.random.rand()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization. \n",
    "As you know, it is necessary to randomly\n",
    "initialize the parameters of your neural network (trying to avoid symme-\n",
    "try and saturating neurons, and ideally so that the pre-activation lies in\n",
    "the bending region of the activation function so that the overall networks\n",
    "acts as a non linear function). We suggest that you sample the weights\n",
    "of a layer from a uniform distribution in [ -1/sqrt(n_c), 1/sqrt(n_c) ], \n",
    "where n c is the number of inputs for this layer (changing from one layer to the other).\n",
    "Biases can be initialized at 0. Justify any other initialization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fprop\n",
    "fprop will compute the forward progpagation i.e. step by step computation from the input to the output and the cost of the activations of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bprop\n",
    "bprop will use the computed activations by fprop and does\n",
    "the backpropagation of the gradients from the cost to the input following\n",
    "precisely the steps derived in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference gradient check. We can estimate the gradient nu-\n",
    "merically using the finite difference method. You will implement this estimate as a tool to check your gradient computation. To do so, calculate the value of the loss function for the current parameter values (for a single example or a mini batch). Then for each scalar parameter θ k , change the\n",
    "parameter value by adding a small perturbation (10^−6 < ε < 10^−4 )\n",
    "and calculate the new value of the loss (same example or minibatch),\n",
    "then set the value of the parameter back to its original value. The partial\n",
    "derivative with respect to this parameter is estimated by dividing the\n",
    "change in the loss function by ε. The ratio of your gradient computed\n",
    "by backpropagation and your estimate using finite difference should be\n",
    "between 0.99 and 1.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in one : Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def plot_function(self, train_data, title):\n",
    "        plt.figure()\n",
    "        d1 = train_data[train_data[:, -1] > 0]\n",
    "        d2 = train_data[train_data[:, -1] == 0]\n",
    "        plt.scatter(d1[:, 0], d1[:, 1], c='b', label='classe 1')\n",
    "        plt.scatter(d2[:, 0], d2[:, 1], c='g', label='classe 0')\n",
    "        x = np.linspace(np.min(train_data[:, 0]) - 0.5,\n",
    "                        np.max(train_data[:, 0]) + 0.5,\n",
    "                        100)\n",
    "        y = -(self.weights[0]*x + self.bias - .5)/self.weights[1]\n",
    "        plt.plot(x, y, c='r', lw=2, label='y = -(w1*x + b1)/w2')\n",
    "        plt.xlim(np.min(train_data[:, 0]) - 0.5, np.max(train_data[:, 0]) + 0.5)\n",
    "        plt.ylim(np.min(train_data[:, 1]) - 0.5, np.max(train_data[:, 1]) + 0.5)\n",
    "        plt.grid()\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "    \n",
    "    def __init__(self, nc, m, k):\n",
    "        #nc : number of neurons in the hidden layer\n",
    "        #m : number of classes\n",
    "        #k : size of mini-batches to use\n",
    "        self.nc = nc\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        \n",
    "        \n",
    "    def ini(self,train_data):\n",
    "        self.train_data = train_data\n",
    "        self.x = train_data[:,:-1]\n",
    "        self.y = train_data[:,-1]\n",
    "        self.b1=np.zeros(self.nc)\n",
    "        self.b2=np.zeros(self.m)\n",
    "        self.nc1 = self.x.shape[1]\n",
    "        self.nc2 = self.nc\n",
    "        self.w1 = np.multiply(np.ones((self.nc,self.nc1)),uniforme(-1/sqrt(self.nc1),1/sqrt(self.nc1)))\n",
    "        self.w2 = np.multiply(np.ones((self.m,self.nc)),uniforme(-1/sqrt(self.nc2),1/sqrt(self.nc2)))\n",
    "        \n",
    "    def fprop(self):\n",
    "        #ha : activation des neurones de la couche cachée\n",
    "        self.ha= np.add(np.dot(self.w1,np.transpose(self.x)),self.b1)\n",
    "        #hs : sortie des neurones de la couche cachée\n",
    "        self.hs= np.maximum(0,self.ha)\n",
    "        #oa : activation des neurones de la couche de sortie\n",
    "        self.oa= np.add(np.dot(self.w2,self.hs),self.b2)\n",
    "        #os : sortie des neurones de la couche cachée\n",
    "        self.os=softmax(self.oa)\n",
    "        #L : fonction de coût\n",
    "        self.L= -np.log(self.os)\n",
    "        print(self.L)\n",
    "        return self.L\n",
    "    \n",
    "    def bprop(self):\n",
    "        #grad_oa : gradient de la fonction d'activation de la couche de sortie par rapport à L\n",
    "        onehot=np.zeros(self.m)\n",
    "        onehot[np.int_(self.y)-1]=1\n",
    "        self.grad_oa = self.os-onehot\n",
    "        #grad_w2 et grad_b2\n",
    "        self.grad_w2 = np.dot(self.grad_oa,np.transpose(self.hs))\n",
    "        self.grad_b2 = self.grad_oa\n",
    "        #grad_hs\n",
    "        self.grad_hs = np.dot(np.transpose(self.w2),self.grad_oa)\n",
    "        #grad_ha\n",
    "        def I_ha(x):\n",
    "            #x is a vector\n",
    "            y=np.zeros(len(x))\n",
    "            for i in range(0, len(x)):\n",
    "                    if x[0,i]>0:\n",
    "                        y[i] = 1\n",
    "                    else:\n",
    "                        y[i]=0\n",
    "            return y\n",
    "\n",
    "        self.grad_ha = np.multiply(self.grad_hs,I_ha(self.ha))\n",
    "        #grad_w1 et grad_b1\n",
    "        self.grad_w1 = np.dot(self.grad_ha,np.transpose(self.x))\n",
    "        self.grad_b1 = self.grad_ha\n",
    "        #grad_x\n",
    "        self.grad_x = np.dot(np.transpose(self.w1),self.grad_ha)\n",
    "        #elastic regularization\n",
    "        self.grad_w2_el = self.grad_w2+2*self.w2-np.sign(self.w2)\n",
    "        self.grad_w1_el = self.grad_w1+2*self.w1-np.sign(self.w1)\n",
    "        #to remove\n",
    "        print(\"self.grad_x=\",self.grad_x)\n",
    "        #print(\"I_ha(x)=\",I_ha(self.ha))\n",
    "        #print(\"self.grad_ha=\",self.grad_ha)\n",
    "        #print(\"self.grad_w1=\",self.grad_w1)\n",
    "        \n",
    "    #check if slightly changed parameters yield a similar gradient\n",
    "    def finite_check(self):\n",
    "        e = 0.00002\n",
    "        #copier le réseau actuel et \n",
    "        net_copy = copy.copy(self)\n",
    "        net_copy.w1 = np.add(self.w1,e)\n",
    "        net_copy.w2 = np.add(self.w2,e)\n",
    "        net_copy.b1 = np.add(self.b1,e)\n",
    "        net_copy.b2 = np.add(self.b2,e)\n",
    "        #gradients avec nouvelles valeurs des paramètres\n",
    "        net_copy.fprop()\n",
    "        net_copy.bprop()\n",
    "        #check difference\n",
    "        ratio_w1 = np.divide(self.grad_w1,net_copy.grad_w1)\n",
    "        print ratio_w1\n",
    "        ratio_w2 = np.divide(self.grad_w2,net_copy.grad_w2)\n",
    "        print ratio_w2\n",
    "        ratio_b1 = np.divide(self.grad_b1,net_copy.grad_b1)\n",
    "        print ratio_b1\n",
    "        ratio_b2 = np.divide(self.grad_b2,net_copy.grad_b2)\n",
    "        print ratio_b2\n",
    "        ratios = [ratio_w1,ratio_w2,ratio_b1,ratio_b2]\n",
    "        #sortie\n",
    "        for ratio in ratios:\n",
    "            if (ratio<0.99).any() or (ratio>1.01).any():\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Size of the mini batches. \n",
    "We ask that your computation and gradient descent is done on minibatches (as opposed to the whole training set)\n",
    "with adjustable size using a hyperparameter K. In the minibatch case,\n",
    "we do not manipulate a single input vector, but rather a batch of input\n",
    "vectors grouped in a matrix (that will give a matrix representation at\n",
    "each layer, and for the input). In the case where the size is one, we obtain\n",
    "an equivalent to the stochastic gradient. Given that numpy is eﬃcient on\n",
    "matrix operations, it is more eﬃcient to perform computations on a whole\n",
    "minibatch. It will greatly impact the execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As a beginning, start with an implementation that computes the gradients\n",
    "for a single example, and check that the gradient is correct using the ﬁnite\n",
    "diﬀerence method described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.69314718  0.69314718]\n",
      " [ 0.69314718  0.69314718]]\n",
      "('self.grad_x=', array([[ 0.07184137, -0.07184137],\n",
      "       [ 0.07184137, -0.07184137]]))\n",
      "[[ 0.69314718  0.69314718]\n",
      " [ 0.69314718  0.69314718]]\n",
      "('self.grad_x=', array([[ 0.07184562, -0.07184562],\n",
      "       [ 0.07184562, -0.07184562]]))\n",
      "[[ 0.99986085]\n",
      " [ 0.99986085]]\n",
      "[[ nan  nan]\n",
      " [ nan  nan]]\n",
      "[[ 0.99986085  0.99986085]\n",
      " [ 0.99986085  0.99986085]]\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul Thillen\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:88: RuntimeWarning: invalid value encountered in divide\n",
      "C:\\Users\\Paul Thillen\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\Paul Thillen\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple x : 2 neurones cachés, 2 dimensions, classe = 1\n",
    "x= np.array([[-1.2084724, 0.39429077, 1.]])\n",
    "net1 = neural_net(2,2,1)\n",
    "net1.ini(x)\n",
    "net1.fprop()\n",
    "net1.bprop()\n",
    "net1.finite_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the gradients for both methods (direct computation and ﬁnite\n",
    "difference) for a small network (e.g. d = 2 and d h = 2) with random\n",
    "weights and for a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Add a hyperparameter for the minibatch size K to allow compute the\n",
    "gradients on a minibatch of K examples (in a matrix), by looping over\n",
    "the K examples (this is a small addition to your previous code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
