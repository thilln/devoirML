{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 2 & 3   \n",
    "# Paul Thillen, Louis-Philippe Noël\n",
    "# 3. PRACTICAL PART (40 pts) : Neural network implementation and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerically stable softmax. \n",
    "You will need to compute a numerically\n",
    "stable softmax. Refer to posted readings to see how to do this. Start by\n",
    "writing the expression for a single vector, then adapt it for a mini-batch\n",
    "of examples stored in a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    maximum = np.max(x, axis=-1, keepdims=True)\n",
    "    top = np.exp(x - maximum)\n",
    "    bottom = np.sum(top, axis=-1, keepdims=True)\n",
    "    return top/bottom\n",
    "\n",
    "#test\n",
    "#A= array([[ 1.,  3.,  4.,  5.,  6.],\n",
    "#       [ 4.,  5.,  6.,  3.,  3.],\n",
    "#       [ 1.,  3.,  4.,  5.,  6.],\n",
    "#       [ 4.,  5.,  6.,  3.,  3.]])\n",
    "#softmax(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization. \n",
    "As you know, it is necessary to randomly\n",
    "initialize the parameters of your neural network (trying to avoid symme-\n",
    "try and saturating neurons, and ideally so that the pre-activation lies in\n",
    "the bending region of the activation function so that the overall networks\n",
    "acts as a non linear function). We suggest that you sample the weights\n",
    "of a layer from a uniform distribution in [ -1/sqrt(n_c), 1/sqrt(n_c) ], \n",
    "where n c is the number of inputs for this layer (changing from one layer to the other).\n",
    "Biases can be initialized at 0. Justify any other initialization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniforme(a,b):\n",
    "return(a+(b-a)*random())\n",
    "\n",
    "def initialisation(x):\n",
    "    #Prend en entrée une couche du réseau et retourne les poids associés\n",
    "    nc = \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fprop\n",
    "fprop will compute the forward progpagation i.e. step by step computation from the input to the output and the cost of the activations of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fprop(x):\n",
    "    #ha : activation des neurones de la couche cachée\n",
    "    ha= np.dot(w1,x)+b1\n",
    "    #hs : sortie des neurones de la couche cachée\n",
    "    hs= np.maximum(0,ha)\n",
    "    #oa : activation des neurones de la couche de sortie\n",
    "    oa= np.dot(w2,hs)+b2\n",
    "    #os : sortie des neurones de la couche cachée\n",
    "    os=softmax(oa)\n",
    "    #L : fonction de coût\n",
    "    L= -np.log(os)\n",
    "    return L\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bprop\n",
    "bprop will use the computed activations by fprop and does\n",
    "the backpropagation of the gradients from the cost to the input following\n",
    "precisely the steps derived in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bprop(x,y,m):\n",
    "    #grad_oa : gradient de la fonction d'activation de la couche de sortie par rapport à L\n",
    "    onehot=np.zeros(m)\n",
    "    onehot[y-1]=1\n",
    "    grad_oa = os-onehot\n",
    "    #grad_w2 et grad_b2\n",
    "    grad_w2 = np.dot(np.transpose(grad_oa),hs)\n",
    "    grad_b2 = grad_oa\n",
    "    #grad_hs\n",
    "    grad_hs = np.dot(np.transpose(w2),grad_oa)\n",
    "    #grad_ha\n",
    "    def I_ha(x):\n",
    "        #x is a vector\n",
    "        y=np.zeros(len(x))\n",
    "        for i in range(0, len(x)):\n",
    "                if x[i]>0:\n",
    "                    y[i] = x\n",
    "                else:\n",
    "                    y[i]=0\n",
    "        return y\n",
    "    \n",
    "    grad_ha = np.dot(grad_hs,I_ha(ha))\n",
    "    #grad_w1 et grad_b1\n",
    "    grad_w1 = np.dot(grad_ha,x)\n",
    "    grad_b1 = grad_ha\n",
    "    #grad_x\n",
    "    grad_x = np.dot(grad_ha,w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference gradient check. We can estimate the gradient nu-\n",
    "merically using the finite difference method. You will implement this estimate as a tool to check your gradient computation. To do so, calculate the value of the loss function for the current parameter values (for a single example or a mini batch). Then for each scalar parameter θ k , change the\n",
    "parameter value by adding a small perturbation (10^−6 < ε < 10^−4 )\n",
    "and calculate the new value of the loss (same example or minibatch),\n",
    "then set the value of the parameter back to its original value. The partial\n",
    "derivative with respect to this parameter is estimated by dividing the\n",
    "change in the loss function by ε. The ratio of your gradient computed\n",
    "by backpropagation and your estimate using finite difference should be\n",
    "between 0.99 and 1.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Size of the mini batches. \n",
    "We ask that your computation and gradient descent is done on minibatches (as opposed to the whole training set)\n",
    "with adjustable size using a hyperparameter K. In the minibatch case,\n",
    "we do not manipulate a single input vector, but rather a batch of input\n",
    "vectors grouped in a matrix (that will give a matrix representation at\n",
    "each layer, and for the input). In the case where the size is one, we obtain\n",
    "an equivalent to the stochastic gradient. Given that numpy is eﬃcient on\n",
    "matrix operations, it is more eﬃcient to perform computations on a whole\n",
    "minibatch. It will greatly impact the execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As a beginning, start with an implementation that computes the gradients\n",
    "for a single example, and check that the gradient is correct using the ﬁnite\n",
    "diﬀerence method described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the gradients for both methods (direct computation and ﬁnite\n",
    "difference) for a small network (e.g. d = 2 and d h = 2) with random\n",
    "weights and for a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Add a hyperparameter for the minibatch size K to allow compute the\n",
    "gradients on a minibatch of K examples (in a matrix), by looping over\n",
    "the K examples (this is a small addition to your previous code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
