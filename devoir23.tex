\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage[utf8]{inputenc}
\usepackage{bbm}
\usepackage{listings}

 
\DeclareMathOperator*{\argmin}{arg\,min}
 
 
\begin{document}
 

 
\title{Assignment 2 \& 3}%replace X with the appropriate number
\author{Paul Thillen et Louis-Philippe NoÃ«l\\ %replace with your name
IFT3395/6390 - Machine learning} %if necessary, replace with your course title
 
\maketitle
 
\section{Theoretical part A}
\subsection{}
To show: 
\begin{align*}
sigmoid(x)=\frac{1}{2} ( tanh(\frac{x}{2})+1)
\end{align*}
Which is equivalent to showing:
\begin{align*}
tanh(x) = 2 \cdot sigmoid(2x)-1
\end{align*}
We have:
\begin{align*}
tanh(x) & = \frac{e^x-e^{-x}}{e^x+e^{-x}} = \frac{e^x-\frac{1}{e^x}}{e^x+\frac{1}{e^x}} = \frac{\frac{e^{2x}-1}{e^x}}{\frac{e^{2x}+1}{e^x}} \\
& = \frac{e^{2x}-1}{e^{2x}+1}
\end{align*}
and:
\begin{align*}
sigmoid(x) &= \frac{1}{1+e^{-x}} =  \frac{1}{1+\frac{1}{e^{x}}} = \frac{1}{\frac{e^x+1}{e^x}} \\
& = \frac{e^x}{e^x+1}
\end{align*}
consequently:
\begin{align*}
2 \cdot sigmoid(2x) - 1 & = 2 \cdot \frac{ e^{2x}}{e^{2x}+1}-1 \\
& = \frac{ 2 \cdot e^{2x}}{e^{2x}+1}- \frac{e^{2x}+1}{e^{2x}+1} \\
& = \frac{e^{2x}-1}{e^{2x}+1} = tanh(x)
\end{align*}
\subsection{}
To show:
\begin{align*}
ln(sigmoid(x)) = -softplus(-x)
\end{align*}
We have:
\begin{align*}
ln(sigmoid(x)) = ln ( \frac{1}{1+e^{-x}} ) = - ln( 1+e^{-x} ) = - softplus(-x)
\end{align*}
\subsection{}
To show:
\begin{align*}
sigmoid'(x) = sigmoid(x) \cdot (1-sigmoid(x))
\end{align*}
We have:
\begin{align*}
sigmoid'(x) & = (\frac{e^x}{1+e^x})' \\
& =  \frac{e^x(1+e^x)-e^x \cdot e^x}{(1+e^x)^2} \\
& = \frac{e^x}{1+e^x} (1-\frac{e^x}{1+e^x}) \\
& = sigmoid(x) \cdot (1-sigmoid(x))
\end{align*}
\subsection{}
To show:
\begin{align*}
tanh'(x) = 1-tanh^2(x)
\end{align*}
We have:
\begin{align*}
tanh'(x) & = (\frac{e^x-e^{-x}}{e^x+e^{-x}})' \\
& = \frac{(e^x+e^{-x})^2-(e^x-e^{-x})^2}{(e^x+e^{-x})^2} \\
& = 1 - \frac{(e^x-e^{-x})^2}{(e^x+e^{-x})^2} \\
& = 1 -tanh^2(x)
\end{align*}
\subsection{Write sign using only indicator functions}
\begin{align*}
sgn(x) & = \mathbbm{1}_\mathbb{R_+}(x) - \mathbbm{1}_\mathbb{R_-}(x)
\end{align*}
\subsection{Derivative of abs}
\begin{align*}
abs(x)=\begin{cases}
               x$, if $ x > 0 \\
               0 $, if $ x = 0 \\
               -x$, if $ x < 0 \\
            \end{cases}
\end{align*}
\begin{align*}
abs'(x)=\begin{cases}
               1$, if $ x > 0 \\
               0 $, if $ x = 0 \\
               -1$, if $ x < 0 \\
            \end{cases}
\end{align*}
\subsection{Derivative of rect}
\begin{align*}
rect(x)=\begin{cases}
               x$, if $ x > 0 \\
               0 $, else$ \\
            \end{cases} = \mathbbm{1}_{\{x>0\}}(x) \cdot x
\end{align*}
\begin{align*}
rect'(x) = \mathbbm{1}_{\{x>0\}}(x)
\end{align*}
\subsection{L2 gradient}
\begin{align*}
\frac{\partial || x ||^2_2}{\partial x} = \begin{pmatrix}
\frac{\partial}{\partial x_1} || x ||^2_2 \\
... \\
\frac{\partial}{\partial x_d} || x ||^2_2 \\
\end{pmatrix} = 
\begin{pmatrix}
2x_1 \\
... \\
2x_d \\
\end{pmatrix}
\end{align*}
\subsection{L1 gradient}
\begin{align*}
\frac{\partial || x ||_1}{\partial x} = \begin{pmatrix}
\frac{\partial}{\partial x_1} || x ||_1 \\
... \\
\frac{\partial}{\partial x_d} || x ||_1 \\
\end{pmatrix} = 
\begin{pmatrix}
abs'(x_1) \\
... \\
abs'(x_d) \\
\end{pmatrix}
\end{align*}

\section{Theoretical part B}
\subsection{}
Dimensions of $W^{(1)}$ and $b^{(1)}$:
\begin{align*}
dim(W^{(1)}) = d_h \times d \\
dim(b^{(1)}) = d_h \\
\end{align*}
Preactivation vector of neurons of the hidden layer $h^a$ where $w_j^{(1)}$ is the $j$-th row of $W^{(1)}$.
\begin{align*}
h^a=W^{(1)} \cdot x + b^{(1)} \\
h^a_j = w_j^{(1)} \cdot x + b^{(1)}_j
\end{align*}
Ouput vector of the hidden layer $h^s$:
\begin{align*}
h^s = rect(h^a)  \\
h^s_k = max(0,h^a_k)
\end{align*}


\subsection{}
Dimensions of $W^{(2)}$ and $b^{(2)}$:
\begin{align*}
dim(W^{(2)}) = m \times d_h \\
dim(b^{(2)}) = m \\
\end{align*}
Preactivation vector of neurons of the output layer $o^a$ where $w_j^{(2)}$ is the $j$-th row of $W^{(2)}$.
\begin{align*}
o^a=W^{(1)} \cdot h^s + b^{(1)} \\
o^a_j = w_j^{(1)} \cdot h^s + b^{(1)}_j
\end{align*}
\subsection{}
Ouput vector of the output layer $o^s$:
\begin{align*}
o^s = softmax(o^a)  \\
o^s_k = \frac{e^{o^a_k}}{\sum\limits_{i=1}^m e^{o^a_i}}
\end{align*}
Since exponentials are always positive and both denominator and numerator are exponentials or sum of exponentials, $o^s_k$ has to be positive too.

If we sum over all k for $o^s_k$, we receive:
\begin{align*}
\sum\limits_{j=1}^m \frac{e^{o^a_j}}{\sum\limits_{i=1}^m e^{o^a_i}} = \frac{\sum\limits_{j=1}^m e^{o^a_j}}{\sum\limits_{i=1}^m} = \frac{\sum\limits_{i=1}^m e^{o^a_i}}{\sum\limits_{i=1}^m e^{o^a_i}} = 1
\end{align*}
These two properties are important because $o^s$ is a probability distribution for each possible class.
\subsection{}
Loss function given a probability $o_y^s(x)$ for a single input vector $x$ to be of class $y$: 
\begin{align*}
L(x,y) & = -log(o_y^s(x)) \\
& = -log(\frac{e^{o^a_y}}{\sum\limits_{i=1}^m e^{o^a_i}}) \\
& = -log(e^{o^a_y}) +log(\sum\limits_{i=1}^m e^{o^a_i}) \\
& = -o^a_y+log(\sum\limits_{i=1}^m e^{o^a_i})
\end{align*}
\subsection{}
What is $\hat{R}$? For a loss function $L$ and training data $D$:
\begin{align*}
\hat{R}(L,D) = \frac{1}{|D|} \sum\limits_d L(x^{(d)},y^{(d)})
\end{align*}
What is $\theta$?
\begin{align*}
\theta = \{W^{(1)},b^{(1)},W^{(2)},b^{(2)}\}
\end{align*}
How many scalar parameters $n_\theta$ are there?
\begin{align*}
n_\theta &= |W^{(1)}|+|b^{(1)}|+|W^{(2)}|+|b^{(2)}| \\
&= d \cdot d_h + d_h + d_h \cdot m + m \\
& = (d_h + 1)d+(m+1)d_h
\end{align*}
Optimization problem:
\begin{align*}
arg min_\theta \hat{R}(L,D) =argmin_\theta \sum\limits_d L(x^{(d)},y^{(d)})
\end{align*}
\subsection{}
Batch gradient descent equation:
\begin{align*}
\theta \leftarrow \theta - \eta \frac{d\hat{R}}{d\theta}
\end{align*}
\subsection{}
To show:
\begin{align}
\label{grad_oa}
\triangledown L({o^a}) = o^s-onehot_m(y)
\end{align}
We have: 
\begin{align*}
\triangledown L({o^a}) & = \begin{pmatrix}
... \\
\frac{d}{do^a_k} -o^a_y+log(\sum\limits_{i=1}^m e^{o^a_i}) \\
... \\
\end{pmatrix} \\
\end{align*}
with
\begin{align*}
\frac{d}{do^a_k} -o^a_y+log(\sum\limits_{i=1}^m e^{o^a_i}) & = \begin{cases}
               -1+\frac{e^{o^a_k}}{\sum\limits_{i=1}^m e^{o^a_i}}$, if $ y = k \\
               0+\frac{e^{o^a_k}}{\sum\limits_{i=1}^m e^{o^a_i}}$, if $ y \neq k \\
            \end{cases} \\
            & = \begin{cases}
               -1+ softmax(o^a_k)$, if $ y = k \\
               softmax(o^a_k)$, if $ y \neq k \\
            \end{cases} \\
\end{align*}
so \begin{align*}
\triangledown L({o^a}) = o^s-onehot_m(y)
\end{align*}
\subsection{}
\begin{lstlisting}
onehot = np.zeros(m)
onehot[y-1] = 1
grad_oa= os - onehot
\end{lstlisting}
\subsection{}
To compute: $\triangledown L(W^{(2)}), \triangledown L(b^{(2)})$.
We know $\frac{d}{do^a_k} L$ and we have with $W_k$ being the $k$-th line of $W$:
\begin{align*}
\frac{d}{dW^{(2)}_{k}} L &= \frac{d}{do^a_k} L \frac{d}{dW^{(2)}_{k}} o^a_k \\
\frac{d}{db^{(2)}_{k}} L &= \frac{d}{do^a_k} L \frac{d}{db^{(2)}_{k}} o^a_k \\
\end{align*}
We have to compute:
\begin{align*}
\frac{d}{dW^{(2)}_{k}} o^a_k & = \frac{d}{dW^{(2)}_{k}} (W^{(2)}_{k} \cdot h^S + b^{(2)}) = h^S \\
\frac{d}{db^{(2)}_{k}} o^a_k &= \frac{d}{dW^{(2)}_{k}} (W^{(2)}_{k} \cdot h^S + b^{(2)}) = 1
\end{align*}
Finally, we have:
\begin{align*}
\frac{d}{dW^{(2)}_{k}} L & = (o^s_k-onehot_m(y)) \cdot h^S \\
\frac{d}{db^{(2)}_{k}} L & = o^s_k-onehot_m(y)
\end{align*}
\subsection{}
In matrix form, we can write:
\begin{align*}
\triangledown L(W^{(2)}) & = \triangledown L(o^a) \cdot (h^S)^T \in m \times d_h \\
\triangledown L(b^{(2)}) & = \triangledown L(o^a) \in m \times 1
\end{align*}
Dimensions: \begin{align*}
\triangledown L(o^a) \in 1 \times m \\
h^S \in 1 \times d_h \\
(h^S)^T \in d_h \times 1 \\
\end{align*}
In Python:
\begin{lstlisting}
grad_b2 = grad_oa
grad_W2 = numpy.dot(grad_oa,numpy.transpose(h_s))
\end{lstlisting}
\subsection{}
To compute: 
\begin{align*}
\frac{d}{dh^s_j}L = \sum\limits_{k=1}^m \frac{dL}{do_k^a} \frac{d}{dh_j^s}o_k^a
\end{align*}
The only unknown is $\frac{d}{dh_j^s}o_k^a$. We have:
\begin{align*}
\frac{d}{dh^s_j} {o^a_k} &= \frac{d}{dh^s_j} (W^{(2)}_{k} \cdot h^S + b^{(2)}) = w^{(2)}_{k,j} \\
\end{align*}
$w_{k,j}$ being the $j$-th column of the $k$-th line of $W$. With the sum, we have:
\begin{align*}
\frac{d}{dh^s_j}L = \sum\limits_{k=1}^m \frac{dL}{do^a_k} \frac{d{o^a_k}}{dh^s_j} = 
\sum\limits_{k=1}^m \frac{dL}{do^a_k}  w^{(2)}_{k,j}=(W^{(2)}_{\bullet,j})^T \frac{dL}{do^a} 
\end{align*}
\subsection{}
In matrix form, we can write :
\begin{align*}
\triangledown L(h^s) =(W^{(2)})^T  \triangledown L(o^a) \in d_h \times 1
\end{align*}
Dimensions :
\begin{align*}
dim((W^{(2)})^T ) = d_h \times m \\
dim(\triangledown L(o^a)) = m \times 1 \\
\end{align*}
\subsection{}
From theoretical part A we know that :
\begin{align*}
rect'(x) = \begin{cases}
               1$, if $ x > 0 \\
               0$, if $ x \leq 0  \\
            \end{cases} = \mathbbm{1}_{\{x>0\}}(x)
\end{align*}
To compute: 
\begin{align*}
\frac{d}{dh^a_j} L =\frac{dL}{dh_j^s} \frac{d}{dh^a_j} h_j^s
\end{align*}
We know $\frac{d}{dh^s_j} L$ and we have:
\begin{align*}
\frac{d}{dh^a_j} {h^s_j} = \frac{d}{dh^a_j} {rect(h^a_j)} &=  \begin{cases}
               1$, if $ h^a_j > 0 \\
               0$, if $ h^a_j \leq 0  \\
            \end{cases}  \\ = \mathbbm{1}_{\{h^a_j>0\}}(h^a_j)
\end{align*}
Finally, we have:
\begin{align*}
\frac{d}{dh^a_j} L & = (W^{(2)}_{\bullet,j})^T \triangledown L(o^a)  \mathbbm{1}_{\{h^a_j>0\}}(h^a_j)
\end{align*}
\subsection{}
In matrix form, we can write with $\odot$ being element-wise multiplication:
\begin{align*}
\triangledown L(h^a) = \triangledown L(h^s) \odot \mathbbm{1}_{\{h^a_j>0\}}(h^a_j) 
\end{align*}
Dimensions :
\begin{align*}
dim(\triangledown L(h^s)) = d_h \times 1 \\
dim(\mathbbm{1}_{\{h^a_j>0\}}(h^a_j)) = d_h \times 1 \\
\end{align*}
\subsection{}
To compute: $\triangledown L(W^{(1)}), \triangledown L(b^{(1)})$.
We know $\frac{d}{dh^a_j} L$ and we have:
\begin{align*}
\frac{d}{dW^{(1)}_j} L &= \frac{d}{dh^a_j} L \frac{d}{dW^{(1)}_j} h^a_j \\
\frac{d}{db^{(1)}_j} L &= \frac{d}{dh^a_j} L \frac{d}{db^{(1)}_j} h^a_j \\
\end{align*}
We have to compute:
\begin{align*}
\frac{d}{dW^{(1)}_j} h^a_j & = \frac{d}{dW^{(1)}_j} (W^{(1)}_j \cdot x + b^{(1)}) = x \\
\frac{d}{db^{(1)}_{j}} h^a_j &= \frac{d}{db^{(1)}} (W^{(1)}_{j} \cdot x + b^{(1)}) = 1
\end{align*}
Finally, we have:
\begin{align*}
\frac{d}{dW^{(1)}_j} L &= \frac{d}{dh^a_j} L \cdot x\\
\frac{d}{db^{(1)}_j} L &= \frac{d}{dh^a_j} L
\end{align*}
\subsection{}
In matrix form, we can write :
\begin{align*}
\triangledown L(W^{(1)})  & =  \triangledown L(h^a) \cdot x^T \in d^h \times d\\
\triangledown L(b^{(1)})  & = \triangledown L(h^a) 
\end{align*}
Dimensions :
\begin{align*}
dim(\triangledown L(h^a)) = d_h \times 1 \\
dim(x) = d \times 1 \\
\end{align*}
\subsection{}
Following the same logic than in question 2.13,we have:
\begin{align*}
\frac{dL}{dx} = \sum\limits_{k=1}^{d_h} \frac{dL}{dh^a_k} \frac{dh^a_k}{dx_j} = (W^{(1)}_{\bullet,j})^T \frac{dL}{dh^a}
\end{align*}
And in matrix form:
\begin{align*}
\triangledown L(x) = (W^{(1)})^T \triangledown L (h^a)
\end{align*}
\subsection{}
We have two parameters: $W$ and $b$. The gradient of b is unchanged. The gradient of W will be affected by the deduction of its sign $(L^1)$ and by the addition of two times its value $(L^2)$ :
\begin{align*} 
\triangledown L(W^{(2)}) & = \triangledown L(o^a) \cdot (h^S)^T + 2 \cdot W^{(2)} - sign(W^{(2)}) \\
\triangledown L(W^{(1)}) & = \triangledown L(h^a) \cdot x^T + 2 \cdot W^{(1)} - sign(W^{(1)}) \\
\end{align*}
\end{document}