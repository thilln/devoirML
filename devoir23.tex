\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage[utf8]{inputenc}
\usepackage{bbm}
\usepackage{listings}

 
\DeclareMathOperator*{\argmin}{arg\,min}
 
 
\begin{document}
 

 
\title{Assignment 2 \& 3}%replace X with the appropriate number
\author{Paul Thillen et Louis-Philippe NoÃ«l\\ %replace with your name
IFT3395/6390 - Machine learning} %if necessary, replace with your course title
 
\maketitle
 
\section{Theoretical part A}
\subsection{}
To show: 
\begin{align*}
sigmoid(x)=\frac{1}{2} ( tanh(\frac{x}{2})+1)
\end{align*}
Which is equivalent to showing:
\begin{align*}
tanh(x) = 2 \cdot sigmoid(2x)-1
\end{align*}
We have:
\begin{align*}
tanh(x) & = \frac{e^x-e^{-x}}{e^x+e^{-x}} = \frac{e^x-\frac{1}{e^x}}{e^x+\frac{1}{e^x}} = \frac{\frac{e^{2x}-1}{e^x}}{\frac{e^{2x}+1}{e^x}} \\
& = \frac{e^{2x}-1}{e^{2x}+1}
\end{align*}
and:
\begin{align*}
sigmoid(x) &= \frac{1}{1+e^{-x}} =  \frac{1}{1+\frac{1}{e^{x}}} = \frac{1}{\frac{e^x+1}{e^x}} \\
& = \frac{e^x}{e^x+1}
\end{align*}
consequently:
\begin{align*}
2 \cdot sigmoid(2x) - 1 & = 2 \cdot \frac{ e^{2x}}{e^{2x}+1}-1 \\
& = \frac{ 2 \cdot e^{2x}}{e^{2x}+1}- \frac{e^{2x}+1}{e^{2x}+1} \\
& = \frac{e^{2x}-1}{e^{2x}+1} = tanh(x)
\end{align*}
\subsection{}
To show:
\begin{align*}
ln(sigmoid(x)) = -softplus(-x)
\end{align*}
We have:
\begin{align*}
ln(sigmoid(x)) = ln ( \frac{1}{1+e^{-x}} ) = - ln( 1+e^{-x} ) = - softplus(-x)
\end{align*}
\subsection{}
To show:
\begin{align*}
sigmoid'(x) = sigmoid(x) \cdot (1-sigmoid(x))
\end{align*}
We have:
\begin{align*}
sigmoid'(x) & = (\frac{e^x}{1+e^x})' \\
& =  \frac{e^x(1+e^x)-e^x \cdot e^x}{(1+e^x)^2} \\
& = \frac{e^x}{1+e^x} (1-\frac{e^x}{1+e^x}) \\
& = sigmoid(x) \cdot (1-sigmoid(x))
\end{align*}
\subsection{}
To show:
\begin{align*}
tanh'(x) = 1-tanh^2(x)
\end{align*}
We have:
\begin{align*}
tanh'(x) & = (\frac{e^x-e^{-x}}{e^x+e^{-x}})' \\
& = \frac{(e^x+e^{-x})^2-(e^x-e^{-x})^2}{(e^x+e^{-x})^2} \\
& = 1 - \frac{(e^x-e^{-x})^2}{(e^x+e^{-x})^2} \\
& = 1 -tanh^2(x)
\end{align*}
\subsection{Write sign using only indicator functions}
\begin{align*}
sgn(x) & = \mathbbm{1}_\mathbb{R_+}(x) - \mathbbm{1}_\mathbb{R_-}(x)
\end{align*}
\subsection{Derivative of abs}
\begin{align*}
abs(x)=\begin{cases}
               x$, if $ x > 0 \\
               0 $, if $ x = 0 \\
               -x$, if $ x < 0 \\
            \end{cases}
\end{align*}
\begin{align*}
abs'(x)=\begin{cases}
               1$, if $ x > 0 \\
               0 $, if $ x = 0 \\
               -1$, if $ x < 0 \\
            \end{cases}
\end{align*}
\subsection{Derivative of rect}
\begin{align*}
rect(x)=\begin{cases}
               x$, if $ x > 0 \\
               0 $, else$ \\
            \end{cases} = \mathbbm{1}_{\{x>0\}}(x) \cdot x
\end{align*}
\begin{align*}
rect'(x) = \mathbbm{1}_{\{x>0\}}(x)
\end{align*}
\subsection{L2 gradient}
\begin{align*}
\frac{\partial || x ||^2_2}{\partial x} = \begin{pmatrix}
\frac{\partial}{\partial x_1} || x ||^2_2 \\
... \\
\frac{\partial}{\partial x_d} || x ||^2_2 \\
\end{pmatrix} = 
\begin{pmatrix}
2x_1 \\
... \\
2x_d \\
\end{pmatrix}
\end{align*}
\subsection{L1 gradient}
\begin{align*}
\frac{\partial || x ||_1}{\partial x} = \begin{pmatrix}
\frac{\partial}{\partial x_1} || x ||_1 \\
... \\
\frac{\partial}{\partial x_d} || x ||_1 \\
\end{pmatrix} = 
\begin{pmatrix}
abs'(x_1) \\
... \\
abs'(x_d) \\
\end{pmatrix}
\end{align*}

\section{Theoretical part B}
\subsection{}
Dimensions of $W^{(1)}$ and $b^{(1)}$:
\begin{align*}
dim(W^{(1)}) = d_h \times d \\
dim(b^{(1)}) = d_h \\
\end{align*}
Preactivation vector of neurons of the hidden layer $h^a$ where $w_j^{(1)}$ is the $j$-th row of $W^{(1)}$.
\begin{align*}
h^a=W^{(1)} \cdot x + b^{(1)} \\
h^a_j = w_j^{(1)} \cdot x + b^{(1)}_j
\end{align*}
Ouput vector of the hidden layer $h^s$:
\begin{align*}
h^s = rect(h^a)  \\
h^s_k = max(0,h^a_k)
\end{align*}


\subsection{}
Dimensions of $W^{(2)}$ and $b^{(2)}$:
\begin{align*}
dim(W^{(2)}) = m \times d_h \\
dim(b^{(2)}) = m \\
\end{align*}
Preactivation vector of neurons of the output layer $o^a$ where $w_j^{(2)}$ is the $j$-th row of $W^{(2)}$.
\begin{align*}
o^a=W^{(1)} \cdot h^s + b^{(1)} \\
o^a_j = w_j^{(1)} \cdot h^s + b^{(1)}_j
\end{align*}
\subsection{}
Ouput vector of the output layer $o^s$:
\begin{align*}
o^s = softmax(o^a)  \\
o^s_k = \frac{e^{o^a_k}}{\sum\limits_{i=1}^m e^{o^a_i}}
\end{align*}
Since exponentials are always positive and both denominator and numerator are exponentials or sum of exponentials, $o^s_k$ has to be positive too.

If we sum over all k for $o^s_k$, we receive:
\begin{align*}
\sum\limits_{j=1}^m \frac{e^{o^a_j}}{\sum\limits_{i=1}^m e^{o^a_i}} = \frac{\sum\limits_{j=1}^m e^{o^a_j}}{\sum\limits_{i=1}^m} = \frac{\sum\limits_{i=1}^m e^{o^a_i}}{\sum\limits_{i=1}^m e^{o^a_i}} = 1
\end{align*}
These two properties are important because $o^s$ is a probability distribution for each possible class.
\subsection{}
Loss function given a probability $o_y^s(x)$ for a single input vector $x$ to be of class $y$: 
\begin{align*}
L(x,y) & = -log(o_y^s(x)) \\
& = -log(\frac{e^{o^a_y}}{\sum\limits_{i=1}^m e^{o^a_i}}) \\
& = -log(e^{o^a_y}) +log(\sum\limits_{i=1}^m e^{o^a_i}) \\
& = -o^a_y+log(\sum\limits_{i=1}^m e^{o^a_i})
\end{align*}
\subsection{}
What is $\hat{R}$? For a loss function $L$ and training data $D$:
\begin{align*}
\hat{R}(L,D) = \frac{1}{|D|} \sum\limits_d L(x^{(d)},y^{(d)})
\end{align*}
What is $\theta$?
\begin{align*}
\theta = \{W^{(1)},b^{(1)},W^{(2)},b^{(2)}\}
\end{align*}
How many scalar parameters $n_\theta$ are there?
\begin{align*}
n_\theta &= |W^{(1)}|+|b^{(1)}|+|W^{(2)}|+|b^{(2)}| \\
&= d \cdot d_h + d_h + d_h \cdot m + m \\
& = (d_h + 1)d+(m+1)d_h
\end{align*}
Optimization problem:
\begin{align*}
arg min_\theta \hat{R}(L,D) =argmin_\theta \sum\limits_d L(x^{(d)},y^{(d)})
\end{align*}
\subsection{}
Batch gradient descent equation:
\begin{align*}
\theta \leftarrow \theta - \eta \frac{d\hat{R}}{d\theta}
\end{align*}
\subsection{}
To show:
\begin{align}
\label{grad_oa}
\triangledown L({o^a}) = o^s-onehot_m(y)
\end{align}
We have: 
\begin{align*}
\triangledown L({o^a}) & = \begin{pmatrix}
... \\
\frac{d}{do^a_k} -o^a_y+log(\sum\limits_{i=1}^m e^{o^a_i}) \\
... \\
\end{pmatrix} \\
\end{align*}
with
\begin{align*}
\frac{d}{do^a_k} -o^a_y+log(\sum\limits_{i=1}^m e^{o^a_i}) & = \begin{cases}
               -1+\frac{e^{o^a_k}}{\sum\limits_{i=1}^m e^{o^a_i}}$, if $ y = k \\
               0+\frac{e^{o^a_k}}{\sum\limits_{i=1}^m e^{o^a_i}}$, if $ y \neq k \\
            \end{cases} \\
            & = \begin{cases}
               -1+ softmax(o^a_k)$, if $ y = k \\
               softmax(o^a_k)$, if $ y \neq k \\
            \end{cases} \\
\end{align*}
so \begin{align*}
\triangledown L({o^a}) = o^s-onehot_m(y)
\end{align*}
\subsection{}
\begin{lstlisting}
onehot = np.zeros(m)
onehot[y-1] = 1
grad_oa= os - onehot
\end{lstlisting}
\subsection{}
To compute: $\triangledown L(W^{(2)}), \triangledown L(b^{(2)})$.
We know $\frac{d}{do^a_k} L$ and we have:
\begin{align*}
\frac{d}{dW^{(2)}_{k}} L &= \frac{d}{do^a_k} L \frac{d}{dW^{(2)}_{k}} o^a_k \\
\frac{d}{db^{(2)}_{k}} L &= \frac{d}{do^a_k} L \frac{d}{db^{(2)}_{k}} o^a_k \\
\end{align*}
We have to compute:
\begin{align*}
\frac{d}{dW^{(2)}_{k}} o^a_k & = \frac{d}{dW^{(2)}_{k}} (W^{(2)}_{k} \cdot h^S + b^{(2)}) = h^S \\
\frac{d}{db^{(2)}_{k}} o^a_k &= \frac{d}{dW^{(2)}_{k}} (W^{(2)}_{k} \cdot h^S + b^{(2)}) = 1
\end{align*}
Finally, we have:
\begin{align*}
\frac{d}{dW^{(2)}_{k}} L & = (o^s_k-onehot_m(y)) \cdot h^S \\
\frac{d}{db^{(2)}_{k}} L & = o^s_k-onehot_m(y)
\end{align*}
\subsection{}
In matrix form, we can write:
\begin{align*}
\triangledown L(W^{(2)}) & = (o^s-onehot_m(y))^T \cdot h^S \\
\triangledown L(b^{(2)}) & = o^s-onehot_m(y)
\end{align*}
In Python:
\begin{lstlisting}
grad_b2 = grad_oa
grad_W2 = numpy.dot(numpy.transpose(grad_oa),h_s)
\end{lstlisting}
\subsection{}
To compute: $\triangledown L(h^s_j)$.
We know $\frac{d}{do^a_k} L$ and we have:
\begin{align*}
\frac{d}{dh^s_j} {o^a_k} &= \frac{d}{dh^s_j} (W^{(2)}_{j} \cdot h^S + b^{(2)}) = W^{(2)}_{j} \\
\end{align*}
With the sum, we have:
\begin{align*}
\triangledown L(h^s_j) = \sum\limits_{k=1}^m \frac{dL}{do^a_k} \frac{d{o^a_k}}{dh^s_j} = 
\sum\limits_{k=1}^m [o^s_k - onehot_m(y)]  W^{(2)}_{k,j}
\end{align*}
\subsection{}
In matrix form, we can write :
\begin{align*}
\triangledown L(h^s_j) =(W^{(2)}_j)^T [o^s - onehot_m(y)]
\end{align*}
Dimensions :
\begin{align*}
dim(W^{(2)}_j ) = 1 \times d \\
dim([o^s - onehot_m(y)]) = 1 \times j \\
\end{align*}
\subsection{}
Let's start by differentiating rect(z) :
\begin{align*}
\frac{d}{dz} {rect(z)} = rect'(z) = \begin{cases}
               h^a_j$, if $ h^a_j > 0 \\
               0$, if $ h^a_j \leq 0  \\
            \end{cases} 
\end{align*}
To compute: $\triangledown L(h^a_j)$.
We know $\frac{d}{dh^s_j} L$ and we have:
\begin{align*}
\frac{d}{dh^a_j} {h^s_j} &=  \begin{cases}
               h^a_j$, if $ h^a_j > 0 \\
               0$, if $ h^a_j \leq 0  \\
            \end{cases}  \\
\end{align*}
Finally, we have:
\begin{align*}
\frac{d}{dh^a_j} L & = \sum\limits_{k=1}^m [o^s_k - onehot_m(y)]  W^{(2)}_{k,j} \times I_{h^a_j > 0}
\end{align*}
\subsection{}
In matrix form, we can write :
\begin{align*}
\triangledown L(h^a_j) =(W^{(2)}_j)^T [o^s - onehot_m(y)] \times I_{h^a_j > 0}
\end{align*}
Dimensions :
\begin{align*}
dim(W^{(2)}_j ) = 1 \times d \\
dim([o^s - onehot_m(y)]) = 1 \times j \\
dim(I_{h^a_j > 0}) = 1 
\end{align*}
\subsection{}
To compute: $\triangledown L(W^{(1)}), \triangledown L(b^{(1)})$.
We know $\frac{d}{dh^a_j} L$ and we have:
\begin{align*}
\frac{d}{dW^{(1)}} L &= \frac{d}{dh^a_j} L \frac{d}{dW^{(1)}} h^a_j \\
\frac{d}{db^{(1)}} L &= \frac{d}{dh^a_j} L \frac{d}{db^{(1)}} h^a_j \\
\end{align*}
We have to compute:
\begin{align*}
\frac{d}{dW^{(1)}} h^a_j & = \frac{d}{dW^{(1)}} (W^{(1)} \cdot X + b^{(1)}) = X \\
\frac{d}{db^{(1)}_{k}} h^a_j &= \frac{d}{db^{(1)}} (W^{(1)}_{k} \cdot X + b^{(1)}) = 1
\end{align*}
Finally, we have:
\begin{align*}
\frac{d}{dW^{(1)}} L & =  \sum\limits_{k=1}^m [o^s_k - onehot_m(y)]  W^{(2)}_{k,j} \times I_{h^a_j > 0} \cdot X \\
\frac{d}{db^{(1)}} L & =   \sum\limits_{k=1}^m [o^s_k - onehot_m(y)]  W^{(2)}_{k,j} \times I_{h^a_j > 0} \\
\end{align*}
\subsection{}
In matrix form, we can write :
\begin{align*}
\triangledown L(W^{(1)}) =(W^{(2)}_j)^T [o^s - onehot_m(y)] \times I_{h^a_j > 0} \cdot X \\
\triangledown L(b^{(1)}) =(W^{(2)}_j)^T [o^s - onehot_m(y)] \times I_{h^a_j > 0} \\
\end{align*}
Dimensions :
\begin{align*}
dim(W^{(2)}_j ) = 1 \times d \\
dim([o^s - onehot_m(y)]) = 1 \times j \\
dim(I_{h^a_j > 0}) = 1 \\
dim(X) = i \times d \\
\end{align*}
\subsection{}
The gradient of L by X is :
\begin{align*}
\triangledown L(X) = \sum\limits_{k=1}^m \frac{dL}{dh^a_k} \frac{d{h^a_k}}{dX} \\
\end{align*}
We know $\frac{d}{dh^a_k} L$ and we have:
\begin{align*}
\frac{d}{dX} h^a_k & = \frac{d}{dX} (W^{(1)}_k \cdot X + b^{(1)}) = W^{(1)}_k \\
\end{align*}
Finally, we have:
\begin{align*}
\frac{d}{dX} L &  = \sum\limits_{k=1}^m [o^s_k - onehot_m(y)]  W^{(2)}_{k} \times I_{h^a_j > 0} \cdot W^{(1)}_{k} \\
\end{align*}
\subsection{}
We have two parameters: W and b. The gradient of b is unchanged. The gradient of W will be affected by the deduction of its sign $(L^1)$ and by the addition of two times its value $(L^2)$ :
\begin{align*}
\frac{d}{dW^{(2)}_{k}} L & = (o^s_k-onehot_m(y)) \cdot h^S + 2 \times W^{(2)}_k - sign(W^{(2)}_k)\\ 
\frac{d}{dW^{(1)}} L & =  \sum\limits_{k=1}^m [o^s_k - onehot_m(y)]  W^{(2)}_{k,j} \times I_{h^a_j > 0} \cdot X+ 2 \times W^{(1)}_k - sign(W^{(1)}_k) \\
\end{align*}
\end{document}