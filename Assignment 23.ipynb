{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 2 & 3\n",
    "# Paul Thillen, Louis-Philippe Noël\n",
    "# 3. PRACTICAL PART (40 pts) : Neural network implementation and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerically stable softmax. \n",
    "You will need to compute a numerically\n",
    "stable softmax. Refer to posted readings to see how to do this. Start by\n",
    "writing the expression for a single vector, then adapt it for a mini-batch\n",
    "of examples stored in a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a449b0a10915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoftmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "softmax = np.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization. \n",
    "As you know, it is necessary to randomly\n",
    "initialize the parameters of your neural network (trying to avoid symme-\n",
    "try and saturating neurons, and ideally so that the pre-activation lies in\n",
    "the bending region of the activation function so that the overall networks\n",
    "acts as a non linear function). We suggest that you sample the weights\n",
    "of a layer from a uniform distribution in [ -1/sqrt(n_c), 1/sqrt(n_c) ], \n",
    "where n c is the number of inputs for this layer (changing from one layer to the other).\n",
    "Biases can be initialized at 0. Justify any other initialization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fprop and bprop. \n",
    "We suggest you implement methods fprop and bprop.\n",
    "fprop will compute the forward progpagation i.e. step by step compu-\n",
    "tation from the input to the output and the cost, of the activations of\n",
    "each layer. bprop will use the computed activations by fprop and does\n",
    "the backpropagation of the gradients from the cost to the input following\n",
    "precisely the steps derived in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference gradient check. We can estimate the gradient nu-\n",
    "merically using the finite difference method. You will implement this estimate as a tool to check your gradient computation. To do so, calculate the value of the loss function for the current parameter values (for a single example or a mini batch). Then for each scalar parameter θ k , change the\n",
    "parameter value by adding a small perturbation (10^−6 < ε < 10^−4 )\n",
    "and calculate the new value of the loss (same example or minibatch),\n",
    "then set the value of the parameter back to its original value. The partial\n",
    "derivative with respect to this parameter is estimated by dividing the\n",
    "change in the loss function by ε. The ratio of your gradient computed\n",
    "by backpropagation and your estimate using finite difference should be\n",
    "between 0.99 and 1.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Size of the mini batches. \n",
    "We ask that your computation and gradient descent is done on minibatches (as opposed to the whole training set)\n",
    "with adjustable size using a hyperparameter K. In the minibatch case,\n",
    "we do not manipulate a single input vector, but rather a batch of input\n",
    "vectors grouped in a matrix (that will give a matrix representation at\n",
    "each layer, and for the input). In the case where the size is one, we obtain\n",
    "an equivalent to the stochastic gradient. Given that numpy is eﬃcient on\n",
    "matrix operations, it is more eﬃcient to perform computations on a whole\n",
    "minibatch. It will greatly impact the execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As a beginning, start with an implementation that computes the gradients\n",
    "for a single example, and check that the gradient is correct using the ﬁnite\n",
    "diﬀerence method described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the gradients for both methods (direct computation and ﬁnite\n",
    "difference) for a small network (e.g. d = 2 and d h = 2) with random\n",
    "weights and for a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Add a hyperparameter for the minibatch size K to allow compute the\n",
    "gradients on a minibatch of K examples (in a matrix), by looping over\n",
    "the K examples (this is a small addition to your previous code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
