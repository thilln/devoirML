{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 2 & 3   \n",
    "# Paul Thillen, Louis-Philippe Noël\n",
    "# 3. PRACTICAL PART (40 pts) : Neural network implementation and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerically stable softmax. \n",
    "You will need to compute a numerically\n",
    "stable softmax. Refer to posted readings to see how to do this. Start by\n",
    "writing the expression for a single vector, then adapt it for a mini-batch\n",
    "of examples stored in a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import gzip,pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#two moons \n",
    "two_moons = np.loadtxt(open('2moons.txt','r'))\n",
    "#mnist\n",
    "f=gzip.open('mnist.pkl.gz')\n",
    "mnist=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    maximum = np.max(x, axis=-1, keepdims=True)\n",
    "    top = np.exp(x - maximum)\n",
    "    bottom = np.sum(top, axis=-1, keepdims=True)\n",
    "    return top/bottom\n",
    "\n",
    "#test\n",
    "#A= array([[ 1.,  3.,  4.,  5.,  6.],\n",
    "#       [ 4.,  5.,  6.,  3.,  3.],\n",
    "#       [ 1.,  3.,  4.,  5.,  6.],\n",
    "#       [ 4.,  5.,  6.,  3.,  3.]])\n",
    "#softmax(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniforme(a,b):\n",
    "    return(a+(b-a)*np.random.rand()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization. \n",
    "As you know, it is necessary to randomly\n",
    "initialize the parameters of your neural network (trying to avoid symme-\n",
    "try and saturating neurons, and ideally so that the pre-activation lies in\n",
    "the bending region of the activation function so that the overall networks\n",
    "acts as a non linear function). We suggest that you sample the weights\n",
    "of a layer from a uniform distribution in [ -1/sqrt(n_c), 1/sqrt(n_c) ], \n",
    "where n c is the number of inputs for this layer (changing from one layer to the other).\n",
    "Biases can be initialized at 0. Justify any other initialization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fprop\n",
    "fprop will compute the forward progpagation i.e. step by step computation from the input to the output and the cost of the activations of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bprop\n",
    "bprop will use the computed activations by fprop and does\n",
    "the backpropagation of the gradients from the cost to the input following\n",
    "precisely the steps derived in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference gradient check. We can estimate the gradient nu-\n",
    "merically using the finite difference method. You will implement this estimate as a tool to check your gradient computation. To do so, calculate the value of the loss function for the current parameter values (for a single example or a mini batch). Then for each scalar parameter θ k , change the\n",
    "parameter value by adding a small perturbation (10^−6 < ε < 10^−4 )\n",
    "and calculate the new value of the loss (same example or minibatch),\n",
    "then set the value of the parameter back to its original value. The partial\n",
    "derivative with respect to this parameter is estimated by dividing the\n",
    "change in the loss function by ε. The ratio of your gradient computed\n",
    "by backpropagation and your estimate using finite difference should be\n",
    "between 0.99 and 1.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in one : Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def plot_function(self, train_data, title):\n",
    "        plt.figure()\n",
    "        d1 = train_data[train_data[:, -1] > 0]\n",
    "        d2 = train_data[train_data[:, -1] == 0]\n",
    "        plt.scatter(d1[:, 0], d1[:, 1], c='b', label='classe 1')\n",
    "        plt.scatter(d2[:, 0], d2[:, 1], c='g', label='classe 0')\n",
    "        x = np.linspace(np.min(train_data[:, 0]) - 0.5,\n",
    "                        np.max(train_data[:, 0]) + 0.5,\n",
    "                        100)\n",
    "        y = -(self.weights[0]*x + self.bias - .5)/self.weights[1]\n",
    "        plt.plot(x, y, c='r', lw=2, label='y = -(w1*x + b1)/w2')\n",
    "        plt.xlim(np.min(train_data[:, 0]) - 0.5, np.max(train_data[:, 0]) + 0.5)\n",
    "        plt.ylim(np.min(train_data[:, 1]) - 0.5, np.max(train_data[:, 1]) + 0.5)\n",
    "        plt.grid()\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "    \n",
    "    def __init__(self, nc, m, k):\n",
    "        #nc : number of neurons in the hidden layer\n",
    "        #m : number of classes\n",
    "        #k : size of mini-batches to use\n",
    "        self.nc = nc\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        \n",
    "        \n",
    "    def ini(self,train_data):\n",
    "        self.train_data = train_data\n",
    "        self.x = np.transpose(train_data[:,:-1])\n",
    "        self.y = train_data[:,-1]\n",
    "        self.b1=np.zeros(self.nc)\n",
    "        self.b2=np.zeros(self.m)\n",
    "        self.nc1 = self.x.shape[1]\n",
    "        self.nc2 = self.nc\n",
    "        self.w1 = np.multiply(np.ones((self.nc,self.nc1)),uniforme(-1/sqrt(self.nc1),1/sqrt(self.nc1)))\n",
    "        self.w2 = np.multiply(np.ones((self.m,self.nc)),uniforme(-1/sqrt(self.nc2),1/sqrt(self.nc2)))\n",
    "        \n",
    "    def fprop(self):\n",
    "        print x\n",
    "        #ha : activation des neurones de la couche cachée\n",
    "        self.ha= np.add(np.dot(self.w1,np.transpose(self.x)),self.b1)\n",
    "        #hs : sortie des neurones de la couche cachée\n",
    "        self.hs= np.maximum(0,self.ha)\n",
    "        #oa : activation des neurones de la couche de sortie\n",
    "        self.oa= np.add(np.dot(self.w2,self.hs),self.b2)\n",
    "        #os : sortie des neurones de la couche cachée\n",
    "        self.os=softmax(self.oa)\n",
    "        #L : fonction de coût\n",
    "        self.L= -np.log(self.os)\n",
    "        print(self.L)\n",
    "        return self.L\n",
    "    \n",
    "    def bprop(self):\n",
    "        #grad_oa : gradient de la fonction d'activation de la couche de sortie par rapport à L\n",
    "        onehot=np.zeros(self.m)\n",
    "        onehot[np.int_(self.y)-1]=1\n",
    "        self.grad_oa = self.os-onehot\n",
    "        #grad_w2 et grad_b2\n",
    "        self.grad_w2 = np.dot(self.grad_oa,np.transpose(self.hs))\n",
    "        self.grad_b2 = self.grad_oa\n",
    "        #grad_hs\n",
    "        self.grad_hs = np.dot(np.transpose(self.w2),self.grad_oa)\n",
    "        #grad_ha\n",
    "        def I_ha(x):\n",
    "            #x is a vector\n",
    "            y=np.zeros(len(x))\n",
    "            for i in range(0, len(x)):\n",
    "                    if x[0,i]>0:\n",
    "                        y[i] = 1\n",
    "                    else:\n",
    "                        y[i]=0\n",
    "            return y\n",
    "\n",
    "        self.grad_ha = np.multiply(self.grad_hs,I_ha(self.ha))\n",
    "        #grad_w1 et grad_b1\n",
    "        self.grad_w1 = np.dot(self.grad_ha,self.x)\n",
    "        self.grad_b1 = self.grad_ha\n",
    "        #grad_x\n",
    "        self.grad_x = np.dot(np.transpose(self.w1),self.grad_ha)\n",
    "        #elastic regularization\n",
    "        self.grad_w2_el = self.grad_w2+2*self.w2-np.sign(self.w2)\n",
    "        self.grad_w1_el = self.grad_w1+2*self.w1-np.sign(self.w1)\n",
    "        #to remove\n",
    "        print(\"self.grad_x=\",self.grad_x)\n",
    "        #print(\"I_ha(x)=\",I_ha(self.ha))\n",
    "        #print(\"self.grad_ha=\",self.grad_ha)\n",
    "        #print(\"self.grad_w1=\",self.grad_w1)\n",
    "        \n",
    "    #check if slightly changed parameters yield a similar gradient\n",
    "    def finite_check(self):\n",
    "        e = 0.00002\n",
    "        #copier le réseau actuel et \n",
    "        net_copy = copy.copy(self)\n",
    "        net_copy.w1 = np.add(self.w1,e)\n",
    "        net_copy.w2 = np.add(self.w2,e)\n",
    "        net_copy.b1 = np.add(self.b1,e)\n",
    "        net_copy.b2 = np.add(self.b2,e)\n",
    "        #gradients avec nouvelles valeurs des paramètres\n",
    "        net_copy.fprop()\n",
    "        net_copy.bprop()\n",
    "        #check difference\n",
    "        ratio_w1 = np.divide(self.grad_w1,net_copy.grad_w1)\n",
    "        print ratio_w1\n",
    "        ratio_w2 = np.divide(self.grad_w2,net_copy.grad_w2)\n",
    "        print ratio_w2\n",
    "        ratio_b1 = np.divide(self.grad_b1,net_copy.grad_b1)\n",
    "        print ratio_b1\n",
    "        ratio_b2 = np.divide(self.grad_b2,net_copy.grad_b2)\n",
    "        print ratio_b2\n",
    "        ratios = [ratio_w1,ratio_w2,ratio_b1,ratio_b2]\n",
    "        #sortie\n",
    "        for ratio in ratios:\n",
    "            if (ratio<0.99).any() or (ratio>1.01).any():\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Size of the mini batches. \n",
    "We ask that your computation and gradient descent is done on minibatches (as opposed to the whole training set)\n",
    "with adjustable size using a hyperparameter K. In the minibatch case,\n",
    "we do not manipulate a single input vector, but rather a batch of input\n",
    "vectors grouped in a matrix (that will give a matrix representation at\n",
    "each layer, and for the input). In the case where the size is one, we obtain\n",
    "an equivalent to the stochastic gradient. Given that numpy is eﬃcient on\n",
    "matrix operations, it is more eﬃcient to perform computations on a whole\n",
    "minibatch. It will greatly impact the execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As a beginning, start with an implementation that computes the gradients\n",
    "for a single example, and check that the gradient is correct using the ﬁnite\n",
    "diﬀerence method described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2084724   0.39429077  1.        ]]\n",
      "[[ 0.59648971  0.80015685]\n",
      " [ 0.59648971  0.80015685]]\n",
      "('self.grad_x=', array([[ 0.        , -0.46412037]]))\n",
      "[[-1.2084724   0.39429077  1.        ]]\n",
      "[[ 0.59648335  0.80016465]\n",
      " [ 0.59648335  0.80016465]]\n",
      "('self.grad_x=', array([[ 0.        , -0.46411025]]))\n",
      "[[ 1.00005479]\n",
      " [ 1.00005479]]\n",
      "[[ 0.99989125  0.99989125]\n",
      " [ 0.99989125  0.99989125]]\n",
      "[[        nan  1.00005479]\n",
      " [        nan  1.00005479]]\n",
      "[[ 1.0000078  1.0000078]\n",
      " [ 1.0000078  1.0000078]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis-philippenoel/anaconda/envs/python27/lib/python2.7/site-packages/ipykernel_launcher.py:91: RuntimeWarning: invalid value encountered in divide\n",
      "/Users/louis-philippenoel/anaconda/envs/python27/lib/python2.7/site-packages/ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in less\n",
      "/Users/louis-philippenoel/anaconda/envs/python27/lib/python2.7/site-packages/ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple x : 2 neurones cachés, 2 dimensions, classe = 1\n",
    "x= np.array([[-1.2084724, 0.39429077, 1.]])\n",
    "net1 = neural_net(2,2,1)\n",
    "net1.ini(x)\n",
    "net1.fprop()\n",
    "net1.bprop()\n",
    "net1.finite_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-2. Display the gradients for both methods (direct computation and ﬁnite\n",
    "difference) for a small network (e.g. d = 2 and d h = 2) with random\n",
    "weights and for a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-3. Add a hyperparameter for the minibatch size K to allow compute the\n",
    "gradients on a minibatch of K examples (in a matrix), by looping over\n",
    "the K examples (this is a small addition to your previous code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-4. Display the gradients for both methods (direct computation and finite\n",
    "difference) for a small network (e.g. d = 2 and dh = 2) with random\n",
    "weights and for a minibatch with 10 examples (you can use examples from\n",
    "both classes from the dataset 2 moons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-5. Train your neural network using gradient descent on the dataset of the\n",
    "two moons. Plot the decision regions for several different values of the\n",
    "hyperparameters (weight decay, number of hidden units, early stopping)\n",
    "so as to illustrate their effect on the capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-6. As a second step, copy your existing implementation to modify it to a new\n",
    "implementation that will use matrix calculus (instead of a loop) on batches\n",
    "of size K to improve efficiency. Take the matrix expressions in numpy\n",
    "derived in the first part, and adapt them for a minibatch of size\n",
    "K. Show in your report what you have modified (precise the\n",
    "former and new expressions with the shapes of each matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-7. Compare both implementations (with a loop and with matrix calculus)\n",
    "to check that they both give the same values for the gradients on the\n",
    "parameters, first for K = 1, then for K = 10. Display the gradients for\n",
    "both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-8. Time how long takes an epoch on MNIST (1 epoch = 1 full traversal\n",
    "through the whole training set) for K = 100 for both versions (loop over\n",
    "a minibatch and matrix calculus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-9. Adapt your code to compute the error (proportion of misclassified examples)\n",
    "on the training set as well as the total loss on the training set during each\n",
    "epoch of the training procedure, and at the end of each epoch, it computes\n",
    "the error and average loss on the validation set and the test set. Display\n",
    "the 6 corresponding figures (error and average loss on train/valid/test),\n",
    "and write them in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-10. Train your network on the MNIST dataset. Plot the training/valid/test\n",
    "curves (error and loss as a function of the epoch number, corresponding\n",
    "to what you wrote in a file in the last question). Include in your report\n",
    "the curves obtained using your best hyperparameters, i.e. for which you\n",
    "obtained your best error on the validation set. We suggest 2 plots : the\n",
    "first one will plot the error rate (train/valid/test with different colors,\n",
    "precise which color in a legend) and the other one for the averaged loss\n",
    "(on train/valid/test). You should be able to get less than 5% test error.\n",
    "Indicate the values of your best hyperparameters corresponding to the\n",
    "curves. Bonus points are given for a test error of less that 2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
